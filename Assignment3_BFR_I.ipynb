{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePcOUuRVoxj7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def load(file_name):\n",
        "    # data(list of list): [[index, dimensions], [.., ..], ...]\n",
        "    data = []              \n",
        "    fh = open(file_name)\n",
        "    for line in fh:\n",
        "        line = line.strip().split(',')\n",
        "        temp = [int(line[0])]\n",
        "        for feature in line[1:]:\n",
        "            temp.append(float(feature))\n",
        "        data.append(temp)  \n",
        "    return data\n",
        "\n",
        "def initialize_centroids(data, dimension, k):\n",
        "    centroids = np.zeros ((k,dimension))\n",
        "    # centroids = [[0 for _ in range(dimension)] for _ in range(k)]\n",
        "    max_feature_vals = [0 for _ in range(dimension)]\n",
        "    min_feature_vals = [float('inf') for _ in range(dimension)]\n",
        "    for point in data:\n",
        "        for i in range(dimension-1):\n",
        "            max_feature_vals[i] = max(max_feature_vals[i], point[i + 1])\n",
        "            min_feature_vals[i] = min(min_feature_vals[i], point[i + 1])\n",
        "    for i in range(dimension):\n",
        "        min_feature_val = min_feature_vals[i]\n",
        "        max_feature_val = max_feature_vals[i]\n",
        "        diff = max_feature_val - min_feature_val\n",
        "        for j in range(k):\n",
        "            centroids[j][i] = min_feature_val + diff * random.uniform(1e-5, 1)\n",
        "    print(type(centroids))\n",
        "    return centroids\n",
        "\n",
        "def get_euclidean_distance(p1, p2, p1_with_index, p2_with_index):\n",
        "    i1 = 0\n",
        "    i2 = 0\n",
        "    if p1_with_index:\n",
        "        i1 = 1\n",
        "    if p2_with_index:\n",
        "        i2 = 1\n",
        "    sd_sum = 0\n",
        "    for d in range(len(p1) - i1):\n",
        "        sd_sum += (p1[d + i1] - p2[d + i2]) ** 2\n",
        "    return math.sqrt(sd_sum)\n",
        "\n",
        "def get_sample(data):\n",
        "    length = len(data)\n",
        "    sample_size = int(length * 0.01)\n",
        "    random_nums = set()\n",
        "    sample_data = []\n",
        "\n",
        "    for i in range(sample_size):\n",
        "        random_index = random.randint(0, length - 1)\n",
        "        while random_index in random_nums:\n",
        "            random_index = random.randint(0, length - 1)\n",
        "        random_nums.add(random_index)\n",
        "        sample_data.append(data[random_index])\n",
        "    return sample_data\n",
        "\n",
        "\n",
        "def kmeans(data, dimension, k):\n",
        "    \n",
        "    centroids = initialize_centroids(data, dimension, k)\n",
        "    cluster_affiliation = [[tuple(features), None] for features in data]\n",
        "    flag = 1\n",
        "\n",
        "    while flag:\n",
        "\n",
        "        flag=0\n",
        "\n",
        "        for i, point in enumerate(data):\n",
        "            min_distance = float('inf')\n",
        "            min_distance_index = None\n",
        "\n",
        "            #find closest centroids for each data points\n",
        "            for cluster_index, centroid in enumerate(centroids):\n",
        "                if centroid[0] == None:\n",
        "                    continue\n",
        "                distance = get_euclidean_distance(centroid, point, False, True)\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    min_distance_index = cluster_index\n",
        "\n",
        "            #record or update cluster for each data points\n",
        "            if cluster_affiliation[i][1] != min_distance_index:\n",
        "                flag = 1\n",
        "                cluster_affiliation[i][1] = min_distance_index\n",
        "        #recompute centroids\n",
        "        centroids = [[0 for _ in range(dimension)] for _ in range(k)]\n",
        "        clutser_point_count = [0 for _ in range(k)]\n",
        "        for i, point in enumerate(data):\n",
        "            cluster_index = cluster_affiliation[i][1]\n",
        "            clutser_point_count[cluster_index] += 1\n",
        "            for d in range(dimension):\n",
        "                centroids[cluster_index][d] += point[d + 1]\n",
        "        for cluster_index, centroid in enumerate(centroids):\n",
        "            point_count = clutser_point_count[cluster_index]\n",
        "            for d in range(dimension):\n",
        "                if point_count == 0:\n",
        "                    centroids[cluster_index][d] = None\n",
        "                else:\n",
        "                    centroids[cluster_index][d] /= point_count\n",
        "\n",
        "    return (centroids, cluster_affiliation)\n",
        "\n",
        "def gather_clusters_info(centroids, cluster_affiliation):\n",
        "    clusters_temp = [[tuple(centroid), []] for centroid in centroids]\n",
        "    for a in cluster_affiliation:\n",
        "        features, cluster_index = a[0], a[1]\n",
        "        clusters_temp[cluster_index][1].append(features)\n",
        "    clusters = []\n",
        "    for cluster in clusters_temp:\n",
        "        if cluster[0][0] != None:\n",
        "            clusters.append(cluster)\n",
        "    return clusters\n",
        "\n",
        "def delete_redundant_cluster(clusters, final_count):\n",
        "    index_count = []\n",
        "    for index, cluster in enumerate(clusters):\n",
        "        index_count.append((len(cluster[1]), index))\n",
        "    index_count.sort()  #sort with cluster size\n",
        "    for i in range(len(clusters) - final_count):\n",
        "        clusters.pop(0)\n",
        "    return clusters\n",
        "\n",
        "def initialize_stat(clusters, dimension, cluster_min_size):\n",
        "    stats = []\n",
        "    set_point_index = []\n",
        "    remaining_points = []\n",
        "    \n",
        "    #clusters: [(centroid1, [(point1 features), (point2 features), ...]), (centroid2, ...)]\n",
        "\t#centroid: (centroid features)\n",
        "    #points: [(point1 51 features); (point2 51 features) ... ... ..] points in the cluster\n",
        "    for centroid, points in clusters:\n",
        "        if len(points) >= cluster_min_size:\n",
        "            #stat: [0, (tuple 50 features). (tuple 50 features)]\n",
        "            stat = [0, [0 for _ in range(dimension)], [0 for _ in range(dimension)]]\n",
        "            point_index = set()\n",
        "            for point in points:\n",
        "                point_index.add(point[0])\n",
        "                stat[0] += 1\n",
        "                for d in range(dimension):\n",
        "                    stat[1][d] += point[d + 1]\n",
        "                    stat[2][d] += point[d + 1] ** 2\n",
        "                    \n",
        "            # stats: [(numberofpoints in cluster0, SUM(tuple 50 features), SUMSQ (tuple 50 features)); (numberofpoints in cluster1, SUM(tuple 50 features), SUMSQ (tuple 50 features));]\n",
        "            stats.append(stat)\n",
        "            #set_point_index: [{point indeices in cluster 0}, {point indices in cluster 1}]\n",
        "            set_point_index.append(point_index)\n",
        "        else:\n",
        "            remaining_points.extend(points)\n",
        "    return (stats, set_point_index, remaining_points)\n",
        "\n",
        "def get_centroids_sd(stat, dimension):\n",
        "    centroids = []\n",
        "    cluster_sd = []\n",
        "    for N, SUM, SUMSQ in stat:\n",
        "        centroid = []\n",
        "        sd = []\n",
        "        for d in range(dimension):\n",
        "            centroid.append(SUM[d] / N)\n",
        "            sd.append(math.sqrt(SUMSQ[d] / N - (SUM[d] / N) ** 2))\n",
        "        centroids.append(centroid)\n",
        "        cluster_sd.append(sd)\n",
        "    return (centroids, cluster_sd)\n",
        "\n",
        "def get_mahalanobis_distance(p1, p2, p1_with_index, p2_with_index, sd, dimension):\n",
        "    i1 = 0\n",
        "    i2 = 0\n",
        "    if p1_with_index:\n",
        "        i1 = 1\n",
        "    if p2_with_index:\n",
        "        i2 = 1\n",
        "    sum_sq = 0\n",
        "    for d in range(dimension):\n",
        "        sum_sq += ((p1[d + i1] - p2[d + i2]) / sd[d]) ** 2\n",
        "    return math.sqrt(sum_sq)\n",
        "\n",
        "def update_stat(data, stat, set_point_index, dimension, threshold, first_load):\n",
        "    #set_point_index: [{point indeices in cluster 0}, {point indices in cluster 1}]\n",
        "           \n",
        "   \n",
        "    #centroids: [(average  50 tuple); (average  50 tuple); ... ... ]\n",
        "    #cluster_sd = [(cluster0 sd 50 tuple); (cluster1 sd 50 tuple); ... ... ]\n",
        "    centroids, cluster_sd = get_centroids_sd(stat, dimension)\n",
        "\n",
        "    remaining_points = []\n",
        "\n",
        "    for point in data:\n",
        "        point = tuple(point)\n",
        "\n",
        "        if first_load:\n",
        "            point_exist = False\n",
        "            for point_index in set_point_index:\n",
        "                #check whether the point from data already has cluster assignments?\n",
        "                if point[0] in point_index: #search the point in the list point_index for corresponding cluster \n",
        "                    point_exist = True\n",
        "                    break\n",
        "            if point_exist:\n",
        "                continue\n",
        "\n",
        "        min_mahalanobis_distance = float('inf')\n",
        "        for index, centroid in enumerate(centroids):\n",
        "            mahalanobis_distance = get_mahalanobis_distance(point, centroid, True, False, cluster_sd[index], dimension)     \n",
        "            if mahalanobis_distance < min_mahalanobis_distance:\n",
        "                min_mahalanobis_distance = mahalanobis_distance\n",
        "                min_index = index\n",
        "        if min_mahalanobis_distance < threshold:\n",
        "            set_point_index[min_index].add(point[0]) #adding the new point index\n",
        "            stat[min_index][0] += 1 # increase point count\n",
        "            for d in range(dimension):\n",
        "                stat[min_index][1][d] += point[d + 1]\n",
        "                stat[min_index][2][d] += point[d + 1] ** 2\n",
        "        else:\n",
        "            remaining_points.append(point)\n",
        "    return (stat, set_point_index, remaining_points)\n",
        "\n",
        "def merge_clusters(stat1, point_index1, stat2, point_index2, i, j, dimension, combined_cs):\n",
        "    s1 = []\n",
        "    s2 = []\n",
        "    for n in range(3):\n",
        "        s1.append(stat1[i][n])\n",
        "        s2.append(stat2[j][n])\n",
        "    combined_stat = [s1[0] + s2[0]]\n",
        "    v1 = []\n",
        "    v2 = []\n",
        "    for d in range(dimension):\n",
        "        v1.append(s1[1][d] + s2[1][d])\n",
        "        v2.append(s1[2][d] + s2[2][d])\n",
        "    combined_stat.append(v1)\n",
        "    combined_stat.append(v2)\n",
        "    \n",
        "    temp = point_index1[i]\n",
        "    for point in point_index2[j]:\n",
        "        temp.add(point)\n",
        "    if combined_cs:\n",
        "        del stat2[max(i, j)]\n",
        "        del stat2[min(i, j)]\n",
        "        del point_index2[max(i, j)]\n",
        "        del point_index2[min(i, j)]\n",
        "    else:\n",
        "        del stat1[i]\n",
        "        del stat2[j]\n",
        "        del point_index1[i]\n",
        "        del point_index2[j]\n",
        "    stat2.append(combined_stat)\n",
        "    point_index2.append(temp)\n",
        "\n",
        "    return (stat1, point_index1, stat2, point_index2)\n",
        "\n",
        "def check_merge_clusters(stat1, point_index1, stat2, point_index2, dimension, combined_cs, threshold):\n",
        "    while True:\n",
        "        merged = False\n",
        "        \n",
        "        if not combined_cs:\n",
        "            if stat1:\n",
        "                centroids1, cluster_sd1 = get_centroids_sd(stat1, dimension)\n",
        "                centroids2, cluster_sd2 = get_centroids_sd(stat2, dimension)\n",
        "                for i in range(len(stat1)):\n",
        "                    if merged:\n",
        "                        break\n",
        "                    for j in range(len(stat2)):\n",
        "                        mahalanobis_distance = get_mahalanobis_distance(centroids1[i], centroids2[j], False, False, cluster_sd2[j], dimension)\n",
        "                        if mahalanobis_distance < threshold:\n",
        "                            stat1, point_index1, stat2, point_index2 = merge_clusters(stat1, point_index1, stat2, point_index2, i, j, dimension, False)\n",
        "                            merged = True\n",
        "                            break\n",
        "            if not merged:\n",
        "                return (stat1, point_index1, stat2, point_index2)\n",
        "            \n",
        "        else:\n",
        "            cs = stat2\n",
        "            cs_length = len(cs)\n",
        "            if cs_length > 1:\n",
        "                centroids, cluster_sd = get_centroids_sd(cs, dimension)\n",
        "                for i in range(cs_length - 1):\n",
        "                    if merged:\n",
        "                        break\n",
        "                    for j in range(i + 1, cs_length):\n",
        "                        mahalanobis_distance = get_mahalanobis_distance(centroids[i], centroids[j], False, False, cluster_sd[j], dimension)\n",
        "                        if mahalanobis_distance < threshold:\n",
        "                            stat1, point_index1, stat2, point_index2 = merge_clusters(cs, point_index2, cs, point_index2, i, j, dimension, True)\n",
        "                            merged = True\n",
        "                            break\n",
        "            if not merged:\n",
        "                return (stat2, point_index2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\tstart = time.time()\n",
        "\n",
        "\tinputpath =  ''  # sys.argv[1]\n",
        "\tK = 4 #int(sys.argv[2])\n",
        "\toutput1 = 'out1' #sys.argv[3]\n",
        "\toutput2 = 'out2' #sys.argv[4]\n",
        "\n",
        "\tdata_num = 0\n",
        "\tdata = load(inputpath + 'data' + str(data_num) + '.txt')\n",
        "\tdimension = len(data[0]) - 1\n",
        "\tthreshold = 4 * math.sqrt(dimension)\n",
        "\tsample_data = get_sample(data)\n",
        "    #first time run kmeans clustering with K*3 clusters\n",
        "    #centroids: [(centroid1 fearues); (centroid2 features)]\n",
        "    #cluster_affiliation: [((point1 features with point index),group index); ((point2 features with point index),group index)... ]\n",
        "\tcentroids, cluster_affiliation = kmeans(sample_data, dimension, K * 3)\n",
        "\t# if not enough clusters, run kmeans again\n",
        "\twhile True:\n",
        "\t    centroid_count = 0\n",
        "\t    for centroid in centroids:\n",
        "\t        if centroid[0] != None:\n",
        "\t            centroid_count += 1\n",
        "\t    if centroid_count < K:\n",
        "\t        centroids, cluster_affiliation = kmeans(sample_data, dimension, K * 3) #randomize and restart kmeans until get K or more clusters\n",
        "\t    else:\n",
        "\t        break\n",
        "\t# clusters: [(centroid1, [(point1 features), (point2 features), ...]), (centroid2, ...)]\n",
        "\tclusters = gather_clusters_info(centroids, cluster_affiliation)\n",
        "\t# delete redundant clutser\n",
        "\tclusters = delete_redundant_cluster(clusters, K)\n",
        "\n",
        "\t# ds: [(number of points N in cluster 0, [SUM], [SUMSQ]), (number of points N in cluster 1, [SUM], [SUMSQ]), ...] \n",
        "    # ds_point_index:[{cluster0 point indices}, {cluster1 point indices}, .....]\n",
        "\tds, ds_point_index, temp = initialize_stat(clusters, dimension, 1) #based on preliminary clustering\n",
        "\n",
        "\tfirst_load = True\n",
        "\tcs = []\n",
        "\trs = []\n",
        "\tcs_point_index = []\n",
        "\tinter_results = []\n",
        "\n",
        "    #loop begins for different chunks\n",
        "\twhile True:\n",
        "\n",
        "\t\ttry:\n",
        "             # just to check this is the last chunk of data\n",
        "\t\t    data = load(inputpath + '/data' + str(data_num) + '.txt')\n",
        "             \n",
        "\t\texcept:\n",
        "\t\t    cs, cs_point_index, ds, ds_point_index = check_merge_clusters(cs, cs_point_index, ds, ds_point_index, dimension, False, threshold)\n",
        "\t\t    break\n",
        "\n",
        "\t\t# assign points to ds\n",
        "\t\tds, ds_point_index, remaining_points = update_stat(data, ds, ds_point_index, dimension, threshold, first_load) #assign new points to ds \n",
        "        #remaining_points: [{point tuple}, {point tuple}, .....]\n",
        "\t\tif first_load:\n",
        "\t\t    first_load = False  \n",
        "\t\tif remaining_points:    \n",
        "\t\t    if cs:\n",
        "\t\t        # merge cs if needed\n",
        "                # cs: [(number of points N in CScluster 0, [SUM], [SUMSQ]), (number of points N in CS cluster 1, [SUM], [SUMSQ]), ...] \n",
        "                # cs_point_index:[{CScluster0 point indices}, {CScluster1 point indices}, .....]\n",
        "\t\t        cs, cs_point_index = check_merge_clusters(cs, cs_point_index, cs, cs_point_index, dimension, True, threshold)\n",
        "\t\t        # assign points to cs\n",
        "\t\t        cs, cs_point_index, remaining_points = update_stat(remaining_points, cs, cs_point_index, dimension, threshold, False)\n",
        "\t\t    centroids, cluster_affiliation = kmeans(remaining_points + rs, dimension, 3 * K)\n",
        "\t\t    clusters = gather_clusters_info(centroids, cluster_affiliation)\n",
        "\t\t    cs_temp, cs_point_index_temp, rs = initialize_stat(clusters, dimension, 2)\n",
        "\t\t    cs.extend(cs_temp)\n",
        "\t\t    cs_point_index.extend(cs_point_index_temp)\n",
        "\t\tdata_num += 1\n",
        "\t\tds_point_count = 0\n",
        "\t\tcs_point_count = 0\n",
        "\t\tinter_results.append((data_num, len(ds), sum([len(points) for points in ds_point_index]), len(cs), sum([len(points) for points in cs_point_index]), len(rs)))\n",
        "\n",
        "\n",
        "\tresults = {}\n",
        "\tfor index, points in enumerate(ds_point_index):\n",
        "\t    for point in points:\n",
        "\t        results[str(point)] = index\n",
        "\tfor points in cs_point_index:\n",
        "\t    for point in points:\n",
        "\t        results[str(point)] = -1\n",
        "\tfor point in rs:\n",
        "\t    results[str(point[0])] = -1\n",
        "\tfh = open(output1, 'w')\n",
        "\tjson.dump(results, fh)\n",
        "\tfh.close()\n",
        "\t\n",
        "\tfh = open(output2, 'w')\n",
        "\tfh.write('round_id,nof_cluster_discard,nof_point_discard,nof_cluster_compression,nof_point_compression,nof_point_retained')\n",
        "\tfor line in inter_results:\n",
        "\t    fh.write('\\n')\n",
        "\t    fh.write(str(line).strip('()'))\n",
        "\tfh.close()\t\n",
        "\n",
        "\tprint('Duration: %s' % (time.time() - start))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\t\t\t\tmain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "XfWP0XMIt0WK",
        "outputId": "86a3e82b-d932-4d08-9772-8cd5a2bf5f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-1f30df599cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                                 \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-1f30df599cb6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#centroids: [(centroid1 fearues); (centroid2 features)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#cluster_affiliation: [((point1 features with point index),group index); ((point2 features with point index),group index)... ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_affiliation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# if not enough clusters, run kmeans again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-8139eb9e897b>\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(data, dimension, k)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_euclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mmin_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-8139eb9e897b>\u001b[0m in \u001b[0;36mget_euclidean_distance\u001b[0;34m(p1, p2, p1_with_index, p2_with_index)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msd_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msd_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def init():\n",
        "  global data, sample_data\n",
        "  inputpath = ''\n",
        "  data_num = 0\n",
        "  data = load('data' + str(data_num) + '.txt')\n",
        "\n",
        "  sample_data = np.array(get_sample(data))\n",
        "  n = len(sample_data)   # =213\n",
        "  print(str(n) + ' samples loaded.')\n",
        "  d = ( (np.shape(sample_data[0]))[0]  )\n",
        "  print( str(d) + ' dimensions each.')\n",
        "  #print(length + 'datapoints loaded. Sample shape:' + np.shape(sample_data) )\n",
        "\n",
        "  k = 4\n",
        "  centroids = np.array (initialize_centroids (sample_data, d, k))\n",
        "\n",
        "  print(sample_data)\n",
        "#  print(centroids.shape)\n",
        "#  print(centroids)\n",
        "\n",
        "init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE80U5zWscI8",
        "outputId": "5a85ea8d-23eb-4ad0-ae54-f53892f4d121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-78ba4c29e396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#  print(centroids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-78ba4c29e396>\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0minputpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdata_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tuAhAdR3_Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}